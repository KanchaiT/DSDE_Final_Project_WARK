{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- citation_title: string (nullable = true)\n",
      " |-- abstracts: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- affiliations: string (nullable = true)\n",
      " |-- classifications: string (nullable = true)\n",
      " |-- subject_area_name: string (nullable = true)\n",
      " |-- subject_area_code: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      citation_title|           abstracts|             authors|        affiliations|     classifications|   subject_area_name|   subject_area_code|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Public health and...|                   -|Pongpirul Krit; L...|Stanford Universi...|ASJC: 2700; SUBJA...|      Medicine (all)|                2700|\n",
      "|Flexible Printed ...|© 2018 The Instit...|Pratumsiri Teerap...|Chulalongkorn Uni...|ASJC: 2208\\2504; ...|Electrical and El...|          2208; 2504|\n",
      "|Parametric study ...|© 2018 Elsevier L...|Phuakpunk Kiattik...|Chulalongkorn Uni...|CPXCLASS: 522\\723...|Chemistry (all); ...|    1600; 1500; 2209|\n",
      "|Superhydrophobic ...|© 2018 Elsevier B...|Saengkaew Jittrap...|Hirosaki Universi...|CPXCLASS: 641.1\\7...|Chemistry (all); ...|1600; 3104; 3100;...|\n",
      "|Electrochemical i...|© 2018 Elsevier B...|Teengam Prinjapor...|Chulalongkorn Uni...|EMCLASS: 4; ASJC:...|Analytical Chemis...|1602; 1303; 2304;...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with increased memory\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WARK Data Pipeline - Predict Subject Area\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Load CSV data into a Spark DataFrame\n",
    "data_path = \"/Users/tentachita/Downloads/DSDE_Final_Project_WARK/Data_Aj/2/joined_2018-2023.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the data schema\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- citation_title: string (nullable = true)\n",
      " |-- abstracts: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- affiliations: string (nullable = true)\n",
      " |-- classifications: string (nullable = true)\n",
      " |-- subject_area_name: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      citation_title|           abstracts|             authors|        affiliations|     classifications|   subject_area_name|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Public health and...|                   -|Pongpirul Krit; L...|Stanford Universi...|ASJC: 2700; SUBJA...|      Medicine (all)|\n",
      "|Flexible Printed ...|© 2018 The Instit...|Pratumsiri Teerap...|Chulalongkorn Uni...|ASJC: 2208\\2504; ...|Electrical and El...|\n",
      "|Parametric study ...|© 2018 Elsevier L...|Phuakpunk Kiattik...|Chulalongkorn Uni...|CPXCLASS: 522\\723...|Chemistry (all); ...|\n",
      "|Superhydrophobic ...|© 2018 Elsevier B...|Saengkaew Jittrap...|Hirosaki Universi...|CPXCLASS: 641.1\\7...|Chemistry (all); ...|\n",
      "|Electrochemical i...|© 2018 Elsevier B...|Teengam Prinjapor...|Chulalongkorn Uni...|EMCLASS: 4; ASJC:...|Analytical Chemis...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the redundant column\n",
    "df = df.drop('subject_area_code')\n",
    "\n",
    "# Show the data schema\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|features_combined                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |subject_area_name                                                                                                            |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "|Public health and international epidemiology for radiology||-||Pongpirul Krit; Lungren Matthew P.||Stanford University School of Medicine\\Stanford\\United States; Chulalongkorn University\\Bangkok\\Thailand; Bumrungrad International Hospital\\Bangkok\\Thailand; Stanford Healthcare\\Stanford\\United States; Stanford University\\Palo Alto\\United States; Johns Hopkins Bloomberg School of Public Health\\Baltimore\\United States||ASJC: 2700; SUBJABBR: MEDI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |Medicine (all)                                                                                                               |\n",
      "|Flexible Printed Active Antenna for Digital Television Reception||© 2018 The Institute of Electronics, Information and Communication Engineers (IEICE).This paper presents the development of a flexible printed active antenna for the digital television (DTV) reception in areas having poor signal or in high-rise buildings. The antenna structure is composed of a meander line printed on a polyimide film as a radiating element. It has a thickness of 0.3 mm, highly flexible, and very lightweight. The design and analysis of the radiating element are based on a full-wave method implemented by a commercial electromagnetic simulation software. The amplifier circuit consisting of a surface-mount transistor and passive components are integrated directly on the polyimide film, residing next to the feedline, to improve the antenna performance and minimize the whole antenna dimension. The impedance matching between the radiating element, the feed line, and the active circuit are considered. The measured results show that the return loss of more than approximately 10 dB, the maximum gain of about 18 dB, and the Omni-directional radiation pattern are achieved in the operating frequency band of 510-790 MHz. Due to its flexibility, low profile, lightweight, and additional gain, the proposed antenna could be useful for various specific applications.||Pratumsiri Teerapong; Janpugdee Panuwat||Chulalongkorn University\\Bangkok\\Thailand||ASJC: 2208\\2504; CPXCLASS: 402\\714.2\\716\\716.4\\723\\815.1.1; FLXCLASS: 902; SUBJABBR: ENGI\\MATE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |Electrical and Electronic Engineering; Electronic, Optical and Magnetic Materials                                            |\n",
      "|Parametric study of hydrogen production via sorption enhanced steam methane reforming in a circulating fluidized bed riser||© 2018 Elsevier LtdComputational fluid dynamics was applied for sorption enhanced steam methane reforming (SESMR) operating in a circulating fluidized bed (CFB) riser. The solid mixtures consisted of Ni-based catalyst and CaO sorbent. The aim of study was to design a proper pilot-scale CFB riser which produced hydrogen (H2) with both high purity and high flux. The design parameters and the reaction parameters were examined with 2k full factorial design. The significances of each parameter were analyzed by analysis of variance. Using the optimum result, the highest H2 purity reached 98.58% in dry basis accompanied with the highest H2 flux of 0.301 kg/m2 s. The hydrodynamics of this optimum case showed that SESMR was nearly completed since 5.0 m height because axial and radial distributions of solid were well developed without excessive segregation between catalyst and sorbent. Thus, the H2 purity and the H2 flux approached fully developed within the riser height.||Phuakpunk Kiattikhoon; Chalermsinsuwan Benjapon; Putivisutisak Sompong; Assabumrungrat Suttichai||Chulalongkorn University\\Bangkok\\Thailand||CPXCLASS: 522\\723.5\\802.2\\802.3\\803\\804\\804.1; ENCOMPASSCLASS: 306.5.1\\306.6.1\\307.3.1; FLXCLASS: 78.22; ASJC: 1600\\1500\\2209; SUBJABBR: CHEM\\CENG\\ENGI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Chemistry (all); Chemical Engineering (all); Industrial and Manufacturing Engineering                                        |\n",
      "|Superhydrophobic coating from fluoroalkylsilane modified natural rubber encapsulated SiO 2 composites for self-driven oil/water separation||© 2018 Elsevier B.V. A superhydrophobic/superoleophilic mesh was successfully prepared in a simple and environmentally friendly process by coating with fluoroalkylsilane-modified natural rubber-encapsulated silica latex (FAS-modified NR/SiO 2 ). TEM images confirmed the formation of a core-shell morphology, in which the rubber core was fully covered by a silica shell. This improved the thermal stability of the composites. Coating with FAS-modified NR/SiO 2 enhanced both the hydrophobicity and surface roughness of the mesh. The depth profile of the XPS spectra revealed the presence of fluoroalkylsilane on the superhydrophobic mesh and Ar gas ion etching confirmed migration of the fluoroalkylsilane, SiO 2 and carbon to the mesh surface. SEM and AFM results quantified the surface roughness of the coated mesh. Meshes coated with FAS-modified NR/SiO 2 exhibited superhydrophobic/superoleophilic properties. Surfaces coated with these encapsulated particles were successfully applied to oil/water separation. They exhibited a separation efficiency of up to 100% and were reusable across 30 cycles.||Saengkaew Jittraporn; Le Duy; Samart Chanatip; Sawada Hideo; Nishida Masakazu; Chanlek Narong; Kongparakul Suwadee; Kiatkamjornwong Suda||Hirosaki University\\Hirosaki\\Japan; Chulalongkorn University\\Bangkok\\Thailand; Thammasat University\\Pathum Thani\\Thailand; National Institute of Advanced Industrial Science and Technology, Chubu\\Nagoya\\Japan; Synchrotron Light Research Institute (Public Organization)\\Nakhon Ratchasima\\Thailand; Academy of Science\\North Bangkok\\Thailand||CPXCLASS: 641.1\\723.5\\802.3\\813.2\\818.1\\931.2; FLXCLASS: 902; ASJC: 1600\\3104\\3100\\3110\\2508; SUBJABBR: CHEM\\PHYS\\MATE                                                                                                                                                                                                                                                                                                                                                                               |Chemistry (all); Condensed Matter Physics; Physics and Astronomy (all); Surfaces and Interfaces; Surfaces, Coatings and Films|\n",
      "|Electrochemical impedance-based DNA sensor using pyrrolidinyl peptide nucleic acids for tuberculosis detection||© 2018 Elsevier B.V. A label-free electrochemical DNA sensor based on pyrrolidinyl peptide nucleic acid (acpcPNA)-immobilized on a paper-based analytical device (PAD) was developed. Unlike previous PNA-based electrochemical PAD (ePAD) sensors where the capture element was placed directly on the electrode, acpcPNA was covalently immobilized onto partially oxidized cellulose paper allowing regeneration by simple PAD replacement. As an example application, a sensor probe was designed for Mycobacterium tuberculosis (MTB) detection. The ePAD DNA sensor was used to determine a synthetic 15-base oligonucleotide of MTB by measuring the fractional change in the charge transfer resistance (R ct ) obtained from electrochemical impedance spectroscopy (EIS). The R ct of [Fe(CN) 6 ] 3-/4- before and after hybridization with the target DNA could be clearly distinguished. Cyclic voltammetry (CV) was used to verify the EIS results, and showed an increase in peak potential splitting in a similar stepwise manner for each immobilization step. Under optimal conditions, a linear calibration curve in the range of 2–200 nM and the limit of detection 1.24 nM were measured. The acpcPNA probe exhibited very high selectivity for complementary oligonucleotides over single-base-mismatch, two-base-mismatch and non-complementary DNA targets due to the conformationally constrained structure of the acpcPNA. Moreover, the ePAD DNA sensor platform was successfully applied to detect PCR-amplified MTB DNA extracted from clinical samples. The proposed paper-based electrochemical DNA sensor has potential to be an alternative device for low-cost, simple, label-free, sensitive and selective DNA sensor.||Teengam Prinjaporn; Siangproh Weena; Tuantranont Adisorn; Vilaivan Tirayut; Chailapakul Orawon; Henry Charles S.||Chulalongkorn University\\Bangkok\\Thailand; Thailand National Electronics and Computer Technology Center\\Pathum Thani\\Thailand; Colorado State University\\Fort Collins\\United States; Srinakharinwirot University\\Bangkok\\Thailand||EMCLASS: 4; ASJC: 1602\\1303\\2304\\1607; SUBJABBR: CHEM\\BIOC\\ENVI|Analytical Chemistry; Biochemistry; Environmental Chemistry; Spectroscopy                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "\n",
    "# Replace null or missing values\n",
    "df_cleaned = df.fillna({\n",
    "    'citation_title': '-',\n",
    "    'abstracts': '-',\n",
    "    'authors': '-',\n",
    "    'affiliations': '-',\n",
    "    'classifications': '-',\n",
    "    'subject_area_name': '-',\n",
    "})\n",
    "\n",
    "# Combine relevant fields into a single feature\n",
    "df_cleaned = df_cleaned.withColumn(\n",
    "    \"features_combined\", \n",
    "    concat_ws(\"||\", col(\"citation_title\"), col(\"abstracts\"), col(\"authors\"), col(\"affiliations\"), col(\"classifications\"))\n",
    ")\n",
    "\n",
    "# Show the combined features\n",
    "df_cleaned.select(\"features_combined\", \"subject_area_name\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| label|\n",
      "+--------------------+------+\n",
      "|(5000,[1,2,6,10,1...|   1.0|\n",
      "|(5000,[0,1,2,3,4,...|2615.0|\n",
      "|(5000,[0,1,2,3,4,...| 119.0|\n",
      "|(5000,[0,1,2,3,4,...| 191.0|\n",
      "|(5000,[0,1,2,3,4,...| 230.0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Ensure no null values in required columns\n",
    "df_cleaned = df_cleaned.fillna({\"features_combined\": \"-\", \"subject_area_name\": \"-\"})\n",
    "\n",
    "# Tokenize the combined features\n",
    "tokenizer = Tokenizer(inputCol=\"features_combined\", outputCol=\"tokens\")\n",
    "\n",
    "# Convert tokens into numerical features\n",
    "vectorizer = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=5000)\n",
    "\n",
    "# Encode target labels into numerical format\n",
    "label_indexer = StringIndexer(inputCol=\"subject_area_name\", outputCol=\"label\")\n",
    "\n",
    "# Create a preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline(stages=[tokenizer, vectorizer, label_indexer])\n",
    "\n",
    "# Fit and transform the data\n",
    "preprocessed_data = preprocessing_pipeline.fit(df_cleaned).transform(df_cleaned)\n",
    "\n",
    "# Verify the transformed data\n",
    "preprocessed_data.select(\"features\", \"label\").show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| label|\n",
      "+--------------------+------+\n",
      "|(5000,[1,2,6,10,1...|   1.0|\n",
      "|(5000,[0,1,2,3,4,...|2615.0|\n",
      "|(5000,[0,1,2,3,4,...| 119.0|\n",
      "|(5000,[0,1,2,3,4,...| 191.0|\n",
      "|(5000,[0,1,2,3,4,...| 230.0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Count: 1400, Test Data Count: 580\n"
     ]
    }
   ],
   "source": [
    "# Check if features and label columns are present\n",
    "preprocessed_data.select(\"features\", \"label\").show(5)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = preprocessed_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Sample a fraction of the data\n",
    "train_data = train_data.sample(fraction=0.1, seed=42)\n",
    "test_data = test_data.sample(fraction=0.1, seed=42)\n",
    "\n",
    "\n",
    "print(f\"Training Data Count: {train_data.count()}, Test Data Count: {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/08 02:28:15 WARN DAGScheduler: Broadcasting large task binary with size 131.6 MiB\n",
      "[Stage 95:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------+\n",
      "|            features| label|prediction|\n",
      "+--------------------+------+----------+\n",
      "|(5000,[0,1,2,3,4,...|  24.0|       1.0|\n",
      "|(5000,[0,1,2,3,4,...|1860.0|       3.0|\n",
      "|(5000,[0,1,2,3,4,...|  51.0|      28.0|\n",
      "|(5000,[0,1,2,3,4,...| 675.0|      13.0|\n",
      "|(5000,[0,1,2,3,4,...|   1.0|       1.0|\n",
      "+--------------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Train the model\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Display predictions\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/08 02:28:16 WARN DAGScheduler: Broadcasting large task binary with size 131.6 MiB\n",
      "[Stage 96:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/08 02:39:46 WARN TaskSetManager: Stage 125 contains a task of very large size (136858 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model successfully saved.\n",
      "Preprocessing pipeline successfully saved.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "# Ensure lr_model is an instance of LogisticRegressionModel\n",
    "assert isinstance(lr_model, LogisticRegressionModel), \"lr_model is not a LogisticRegressionModel!\"\n",
    "\n",
    "# Save the trained model with overwrite\n",
    "lr_model.write().overwrite().save(\"logistic_regression_model\")\n",
    "print(\"Logistic Regression model successfully saved.\")\n",
    "\n",
    "# Save the preprocessing pipeline with overwrite\n",
    "preprocessing_pipeline.write().overwrite().save(\"preprocessing_pipeline\")\n",
    "print(\"Preprocessing pipeline successfully saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/08 02:54:39 WARN Instrumentation: [be91165c] All labels are the same value and fitIntercept=true, so the coefficients will be zeros. Training is not needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|           features|prediction|\n",
      "+-------------------+----------+\n",
      "|(5,[2,3],[1.0,1.0])|       0.0|\n",
      "+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, StringIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Example: Define your data (this should be your actual data)\n",
    "training_data = spark.createDataFrame([{\n",
    "    \"citation_title\": \"Research in AI\",\n",
    "    \"abstracts\": \"Deep learning advances in AI...\",\n",
    "    \"authors\": \"Author A; Author B\",\n",
    "    \"affiliations\": \"University of XYZ\",\n",
    "    \"classifications\": \"ASJC: 1700; SUBJABBR: COMP\",\n",
    "    \"subject_area_name\": \"AI\"\n",
    "}])\n",
    "\n",
    "# Define your feature transformations and pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"abstracts\", outputCol=\"words\")\n",
    "vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "indexer = StringIndexer(inputCol=\"subject_area_name\", outputCol=\"label\")\n",
    "\n",
    "# Create the pipeline\n",
    "preprocessing_pipeline = Pipeline(stages=[tokenizer, vectorizer, indexer])\n",
    "\n",
    "# Fit the pipeline to your data (this is the model that should be saved)\n",
    "trained_pipeline_model = preprocessing_pipeline.fit(training_data)\n",
    "\n",
    "# Save the trained pipeline model with overwrite\n",
    "trained_pipeline_model.write().overwrite().save(\"preprocessing_pipeline\")\n",
    "\n",
    "# Use the pipeline to transform the data into the format expected by logistic regression\n",
    "training_data_transformed = trained_pipeline_model.transform(training_data)\n",
    "\n",
    "# Now, train your logistic regression model on the transformed data\n",
    "lr_model = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model_trained = lr_model.fit(training_data_transformed)\n",
    "\n",
    "# Save the trained logistic regression model with overwrite\n",
    "lr_model_trained.write().overwrite().save(\"logistic_regression_model\")\n",
    "\n",
    "# --- Later, when you want to load and use the saved models for new data ---\n",
    "\n",
    "# Load the trained preprocessing pipeline model\n",
    "loaded_pipeline = PipelineModel.load(\"preprocessing_pipeline\")\n",
    "\n",
    "# Load the trained logistic regression model\n",
    "loaded_model = LogisticRegressionModel.load(\"logistic_regression_model\")\n",
    "\n",
    "# Process new data\n",
    "new_data = spark.createDataFrame([{\n",
    "    \"citation_title\": \"New Research in AI\",\n",
    "    \"abstracts\": \"Deep learning advances...\",\n",
    "    \"authors\": \"Author A; Author B\",\n",
    "    \"affiliations\": \"University of XYZ\",\n",
    "    \"classifications\": \"ASJC: 1700; SUBJABBR: COMP\",\n",
    "    \"subject_area_name\": \"Unknown\"\n",
    "}])\n",
    "\n",
    "# Transform new data using the loaded preprocessing pipeline\n",
    "new_data_cleaned = loaded_pipeline.transform(new_data)\n",
    "\n",
    "# Get predictions using the loaded model\n",
    "new_predictions = loaded_model.transform(new_data_cleaned)\n",
    "\n",
    "# Show the predictions\n",
    "new_predictions.select(\"features\", \"prediction\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
